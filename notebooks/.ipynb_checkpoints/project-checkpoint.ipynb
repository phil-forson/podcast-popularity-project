{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "427eefa8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace: ['data', 'notebooks']\n",
      "Data: ['categories.json', 'podcasts.json', 'reviews.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Workspace:\", os.listdir(\"/workspace\"))\n",
    "print(\"Data:\", os.listdir(\"/workspace/data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c789be4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/bin/load-spark-env.sh: line 68: ps: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-50e39681-7d96-4823-afaf-f35bf26efdae;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;6.2.0 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-s3;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-kms;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-core;1.12.500 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound software.amazon.ion#ion-java;1.0.2 in central\n",
      "\tfound joda-time#joda-time;2.8.1 in central\n",
      "\tfound com.amazonaws#jmespath-java;1.12.500 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.20.1 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.43.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.43.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.12.0 in central\n",
      "\tfound io.grpc#grpc-context;1.53.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.12.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.0 in central\n",
      "\tfound com.google.api#gax-httpjson;0.108.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.12.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.53.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.53.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.53.0 in central\n",
      "\tfound io.grpc#grpc-core;1.53.0 in central\n",
      "\tfound com.google.api#gax;2.23.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.23.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.api#api-common;2.6.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.9.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.12 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.14.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound io.grpc#grpc-api;1.53.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.53.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.53.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.53.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.53.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.53.0 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound org.jsoup#jsoup;1.18.2 in central\n",
      "\tfound jakarta.mail#jakarta.mail-api;2.1.3 in central\n",
      "\tfound jakarta.activation#jakarta.activation-api;2.1.3 in central\n",
      "\tfound org.eclipse.angus#angus-mail;2.0.3 in central\n",
      "\tfound org.eclipse.angus#angus-activation;2.0.2 in central\n",
      "\tfound org.apache.poi#poi-ooxml;4.1.2 in central\n",
      "\tfound org.apache.poi#poi;4.1.2 in central\n",
      "\tfound org.apache.commons#commons-collections4;4.4 in central\n",
      "\tfound org.apache.commons#commons-math3;3.6.1 in central\n",
      "\tfound com.zaxxer#SparseBitSet;1.2 in central\n",
      "\tfound org.apache.poi#poi-ooxml-schemas;4.1.2 in central\n",
      "\tfound org.apache.xmlbeans#xmlbeans;3.1.0 in central\n",
      "\tfound org.apache.commons#commons-compress;1.19 in central\n",
      "\tfound com.github.virtuald#curvesapi;1.06 in central\n",
      "\tfound org.apache.poi#poi-scratchpad;4.1.2 in central\n",
      "\tfound org.apache.pdfbox#pdfbox;2.0.28 in central\n",
      "\tfound org.apache.pdfbox#fontbox;2.0.28 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-all;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-util-ast;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-util-collection;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-util-misc;0.61.34 in central\n",
      "\tfound org.jetbrains#annotations;15.0 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-util-data;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-util-sequence;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-util-visitor;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-util-builder;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-util-dependency;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-util-format;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-util-html;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-abbreviation;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-util;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-util-options;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-autolink;0.61.34 in central\n",
      "\tfound org.nibor.autolink#autolink;0.6.0 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-admonition;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-anchorlink;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-aside;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-jira-converter;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-gfm-strikethrough;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-tables;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-wikilink;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-ins;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-superscript;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-attributes;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-definition;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-emoji;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-enumerated-reference;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-escaped-character;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-footnotes;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-gfm-issues;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-gfm-tasklist;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-gfm-users;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-gitlab;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-jekyll-front-matter;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-yaml-front-matter;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-jekyll-tag;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-media-tags;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-macros;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-xwiki-macros;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-toc;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-typographic;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-ext-youtube-embedded;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-html2md-converter;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-pdf-converter;0.61.34 in central\n",
      "\tfound com.openhtmltopdf#openhtmltopdf-core;1.0.0 in central\n",
      "\tfound com.openhtmltopdf#openhtmltopdf-pdfbox;1.0.0 in central\n",
      "\tfound org.apache.pdfbox#xmpbox;2.0.16 in central\n",
      "\tfound de.rototor.pdfbox#graphics2d;0.24 in central\n",
      "\tfound com.openhtmltopdf#openhtmltopdf-rtl-support;1.0.0 in central\n",
      "\tfound com.ibm.icu#icu4j;59.1 in central\n",
      "\tfound com.openhtmltopdf#openhtmltopdf-jsoup-dom-converter;1.0.0 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-profile-pegdown;0.61.34 in central\n",
      "\tfound com.vladsch.flexmark#flexmark-youtrack-converter;0.61.34 in central\n",
      "\tfound org.ccil.cowan.tagsoup#tagsoup;1.2.1 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime;1.19.2 in central\n",
      "\tfound com.johnsnowlabs.nlp#jsl-llamacpp-cpu;1.0.2 in central\n",
      "\tfound org.jetbrains#annotations;24.1.0 in central\n",
      "\tfound com.johnsnowlabs.nlp#jsl-openvino-cpu_2.12;0.2.0 in central\n",
      ":: resolution report :: resolve 3393ms :: artifacts dl 134ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-core;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-kms;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-s3;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#jmespath-java;1.12.500 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.github.virtuald#curvesapi;1.06 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.6.2 from central in [default]\n",
      "\tcom.google.api#gax;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.108.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.14.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.9.2 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.20.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.43.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.12 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.12 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.ibm.icu#icu4j;59.1 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#jsl-llamacpp-cpu;1.0.2 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#jsl-openvino-cpu_2.12;0.2.0 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;6.2.0 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime;1.19.2 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.openhtmltopdf#openhtmltopdf-core;1.0.0 from central in [default]\n",
      "\tcom.openhtmltopdf#openhtmltopdf-jsoup-dom-converter;1.0.0 from central in [default]\n",
      "\tcom.openhtmltopdf#openhtmltopdf-pdfbox;1.0.0 from central in [default]\n",
      "\tcom.openhtmltopdf#openhtmltopdf-rtl-support;1.0.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-all;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-abbreviation;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-admonition;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-anchorlink;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-aside;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-attributes;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-autolink;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-definition;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-emoji;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-enumerated-reference;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-escaped-character;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-footnotes;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-gfm-issues;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-gfm-strikethrough;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-gfm-tasklist;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-gfm-users;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-gitlab;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-ins;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-jekyll-front-matter;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-jekyll-tag;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-macros;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-media-tags;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-superscript;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-tables;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-toc;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-typographic;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-wikilink;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-xwiki-macros;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-yaml-front-matter;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-ext-youtube-embedded;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-html2md-converter;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-jira-converter;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-pdf-converter;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-profile-pegdown;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-util;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-util-ast;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-util-builder;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-util-collection;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-util-data;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-util-dependency;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-util-format;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-util-html;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-util-misc;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-util-options;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-util-sequence;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-util-visitor;0.61.34 from central in [default]\n",
      "\tcom.vladsch.flexmark#flexmark-youtrack-converter;0.61.34 from central in [default]\n",
      "\tcom.zaxxer#SparseBitSet;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tde.rototor.pdfbox#graphics2d;0.24 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.53.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjakarta.activation#jakarta.activation-api;2.1.3 from central in [default]\n",
      "\tjakarta.mail#jakarta.mail-api;2.1.3 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\tjoda-time#joda-time;2.8.1 from central in [default]\n",
      "\torg.apache.commons#commons-collections4;4.4 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.19 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.6.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.apache.pdfbox#fontbox;2.0.28 from central in [default]\n",
      "\torg.apache.pdfbox#pdfbox;2.0.28 from central in [default]\n",
      "\torg.apache.pdfbox#xmpbox;2.0.16 from central in [default]\n",
      "\torg.apache.poi#poi;4.1.2 from central in [default]\n",
      "\torg.apache.poi#poi-ooxml;4.1.2 from central in [default]\n",
      "\torg.apache.poi#poi-ooxml-schemas;4.1.2 from central in [default]\n",
      "\torg.apache.poi#poi-scratchpad;4.1.2 from central in [default]\n",
      "\torg.apache.xmlbeans#xmlbeans;3.1.0 from central in [default]\n",
      "\torg.ccil.cowan.tagsoup#tagsoup;1.2.1 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.eclipse.angus#angus-activation;2.0.2 from central in [default]\n",
      "\torg.eclipse.angus#angus-mail;2.0.3 from central in [default]\n",
      "\torg.jetbrains#annotations;24.1.0 from central in [default]\n",
      "\torg.jsoup#jsoup;1.18.2 from central in [default]\n",
      "\torg.nibor.autolink#autolink;0.6.0 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.5 from central in [default]\n",
      "\tsoftware.amazon.ion#ion-java;1.0.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcommons-logging#commons-logging;1.2 by [commons-logging#commons-logging;1.1.3] in [default]\n",
      "\tcommons-codec#commons-codec;1.11 by [commons-codec#commons-codec;1.15] in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.12] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.12] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\tcommons-codec#commons-codec;1.13 by [commons-codec#commons-codec;1.15] in [default]\n",
      "\torg.jetbrains#annotations;15.0 by [org.jetbrains#annotations;24.1.0] in [default]\n",
      "\torg.jsoup#jsoup;1.11.3 by [org.jsoup#jsoup;1.18.2] in [default]\n",
      "\torg.apache.pdfbox#pdfbox;2.0.16 by [org.apache.pdfbox#pdfbox;2.0.28] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |  164  |   0   |   0   |   9   ||  155  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-50e39681-7d96-4823-afaf-f35bf26efdae\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 155 already retrieved (0kB/50ms)\n",
      "25/11/25 04:44:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 8) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RDD count: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"PodcastPopularity\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"com.johnsnowlabs.nlp:spark-nlp_2.12:6.2.0\"\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark version:\", spark.version)\n",
    "print(\"Test RDD count:\", spark.sparkContext.parallelize(range(10)).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8ff6568",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/25 04:45:14 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews rows: 5607021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Podcasts rows: 2077665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:====================================>                     (5 + 3) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories rows: 3706368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "reviews_df    = spark.read.json(\"/workspace/data/reviews.json\")\n",
    "podcasts_df   = spark.read.json(\"/workspace/data/podcasts.json\")\n",
    "categories_df = spark.read.json(\"/workspace/data/categories.json\")\n",
    "\n",
    "print(\"Reviews rows:\", reviews_df.count())\n",
    "print(\"Podcasts rows:\", podcasts_df.count())\n",
    "print(\"Categories rows:\", categories_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42a99e7c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author_id: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- podcast_id: string (nullable = true)\n",
      " |-- rating: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- average_rating: double (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- itunes_id: string (nullable = true)\n",
      " |-- itunes_url: string (nullable = true)\n",
      " |-- podcast_id: string (nullable = true)\n",
      " |-- ratings_count: string (nullable = true)\n",
      " |-- scraped_at: string (nullable = true)\n",
      " |-- slug: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- itunes_id: string (nullable = true)\n",
      " |-- podcast_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_df.printSchema()\n",
    "podcasts_df.printSchema()\n",
    "categories_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be4ca856",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of clean podcasts:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------------+-----------+------------------------------+------------------+\n",
      "|podcast_id                      |title                                                    |description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |avg_rating_platform|rating_count|num_reviews|avg_review_rating_from_reviews|description_length|\n",
      "+--------------------------------+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------------+-----------+------------------------------+------------------+\n",
      "|000c2edf4b922013a27942c565777a78|Jam Session: America’s Jazz Ambassadors Embrace the World|March 20 – August 14, 2011This exceptional collection of photographs and documents drawn from important archives around the country chronicles the tours of American jazz legends as they traveled the globe on behalf of the U.S. State Department. From the mid-1950s through the 1970s, Dizzy Gillespie, Louis Armstrong, Dave Brubeck, Duke Ellington, Benny Goodman, and others served as cultural diplomats. Millions of people experienced these concerts and thrilled to the many styles and variations of the remarkable American art form called jazz music.Jam Session features nearly one hundred captivating images of musicians visiting thirty-five countries on four continents. Along with them, a fully functional stage will be built in the gallery, where live jazz will be performed intermittently throughout the run of exhibition.|4.0                |3           |0          |NULL                          |827               |\n",
      "|0012800db0bfa09856b2de1a0fa15643|CARNE DE VIDEOCLUB                                       |Los 80s y 90s fueron unos años mágicos en los que la llegada y estandarización de los formatos de reproducción de vídeo casero, Vhs y Betamax, creó y cambió los habitos de vida y ocio de varias generaciones que vieron en un negocio creado paralelamente a él la ventana a un mundo de fantasía, aventuras y en ocasiones hasta terror al que solo se podía acceder a través de ese espacio físico al que le guardamos tributo, el Videoclub. Este pretende ser un Podcast mensual sobre el cine que nos apasiono y apasiona, del que repetimos mil y una vez frases, escenas, y gestos en nuestra vida más cotidiana.                                                                                                                                                                                                                                 |5.0                |3           |0          |NULL                          |602               |\n",
      "|0014cfabc59cefd1a31a8affb5732c03|Father Dwight Longenecker - Characters of the Reformation|Fr Dwight Longenecker - Characters of the Reformation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |5.0                |5           |0          |NULL                          |53                |\n",
      "|001aa100906670579283b549d584f5be|Backpacking Light Podcast                                |The Backpacking Light Podcast explores the technology, gear, skills, and philosophy of backcountry wilderness travel through stories, interviews, and investigative reports.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |4.9                |80          |0          |NULL                          |172               |\n",
      "|002543c60c961bebf9e862f8010b90c2|Why We Fly                                               |Why We Fly is a General Aviation podcast sharing pilot stories that illustrate the reasons behind why we choose to fly airplanes. We hope to share the adventure, excitement, and incredible experiences of flying. The goal is to entertain pilots and more importantly to inspire potential pilots to learn more about flying and hopefully one day become pilots themselves. Almost anyone can fly. We are here to share the Why.                                                                                                                                                                                                                                                                                                                                                                                                                       |4.6                |21          |0          |NULL                          |420               |\n",
      "+--------------------------------+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------------+-----------+------------------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean podcasts: 81842\n",
      "Sample of podcast_final:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------------+-----------+------------------------------+------------------+---------------------+------------------+\n",
      "|podcast_id                      |title                                                    |description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |avg_rating_platform|rating_count|num_reviews|avg_review_rating_from_reviews|description_length|category_name        |category_itunes_id|\n",
      "+--------------------------------+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------------+-----------+------------------------------+------------------+---------------------+------------------+\n",
      "|000c2edf4b922013a27942c565777a78|Jam Session: America’s Jazz Ambassadors Embrace the World|March 20 – August 14, 2011This exceptional collection of photographs and documents drawn from important archives around the country chronicles the tours of American jazz legends as they traveled the globe on behalf of the U.S. State Department. From the mid-1950s through the 1970s, Dizzy Gillespie, Louis Armstrong, Dave Brubeck, Duke Ellington, Benny Goodman, and others served as cultural diplomats. Millions of people experienced these concerts and thrilled to the many styles and variations of the remarkable American art form called jazz music.Jam Session features nearly one hundred captivating images of musicians visiting thirty-five countries on four continents. Along with them, a fully functional stage will be built in the gallery, where live jazz will be performed intermittently throughout the run of exhibition.|4.0                |3           |0          |NULL                          |827               |arts                 |478374791         |\n",
      "|0012800db0bfa09856b2de1a0fa15643|CARNE DE VIDEOCLUB                                       |Los 80s y 90s fueron unos años mágicos en los que la llegada y estandarización de los formatos de reproducción de vídeo casero, Vhs y Betamax, creó y cambió los habitos de vida y ocio de varias generaciones que vieron en un negocio creado paralelamente a él la ventana a un mundo de fantasía, aventuras y en ocasiones hasta terror al que solo se podía acceder a través de ese espacio físico al que le guardamos tributo, el Videoclub. Este pretende ser un Podcast mensual sobre el cine que nos apasiono y apasiona, del que repetimos mil y una vez frases, escenas, y gestos en nuestra vida más cotidiana.                                                                                                                                                                                                                                 |5.0                |3           |0          |NULL                          |602               |society-culture      |622395745         |\n",
      "|0012800db0bfa09856b2de1a0fa15643|CARNE DE VIDEOCLUB                                       |Los 80s y 90s fueron unos años mágicos en los que la llegada y estandarización de los formatos de reproducción de vídeo casero, Vhs y Betamax, creó y cambió los habitos de vida y ocio de varias generaciones que vieron en un negocio creado paralelamente a él la ventana a un mundo de fantasía, aventuras y en ocasiones hasta terror al que solo se podía acceder a través de ese espacio físico al que le guardamos tributo, el Videoclub. Este pretende ser un Podcast mensual sobre el cine que nos apasiono y apasiona, del que repetimos mil y una vez frases, escenas, y gestos en nuestra vida más cotidiana.                                                                                                                                                                                                                                 |5.0                |3           |0          |NULL                          |602               |tv-film              |622395745         |\n",
      "|0014cfabc59cefd1a31a8affb5732c03|Father Dwight Longenecker - Characters of the Reformation|Fr Dwight Longenecker - Characters of the Reformation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |5.0                |5           |0          |NULL                          |53                |religion-spirituality|1441058827        |\n",
      "|0014cfabc59cefd1a31a8affb5732c03|Father Dwight Longenecker - Characters of the Reformation|Fr Dwight Longenecker - Characters of the Reformation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |5.0                |5           |0          |NULL                          |53                |spirituality         |1441058827        |\n",
      "+--------------------------------+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------------+-----------+------------------------------+------------------+---------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Podcast_final rows: 171009\n",
      "Filtered category_engagement (top 20):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+------------+-------------+----------------------+-------------------+\n",
      "|category_name                    |num_podcasts|total_reviews|total_ratings_recorded|avg_platform_rating|\n",
      "+---------------------------------+------------+-------------+----------------------+-------------------+\n",
      "|business                         |10535       |119          |131304                |4.787271001423824  |\n",
      "|education                        |10148       |108          |123105                |4.632035869136777  |\n",
      "|health-fitness                   |7243        |59           |102974                |4.695015877398876  |\n",
      "|education-self-improvement       |1333        |59           |24694                 |4.840585146286569  |\n",
      "|arts-visual-arts                 |1815        |55           |22980                 |4.728815426997244  |\n",
      "|business-investing               |1925        |42           |27285                 |4.746025974025973  |\n",
      "|education-how-to                 |709         |42           |10877                 |4.707757404795486  |\n",
      "|society-culture                  |12652       |39           |163138                |4.750276636104959  |\n",
      "|comedy                           |6449        |29           |82668                 |4.81480849744147   |\n",
      "|sports                           |5789        |25           |77852                 |4.751960614959411  |\n",
      "|news                             |4418        |18           |55587                 |4.665142598460846  |\n",
      "|news-tech-news                   |966         |18           |12183                 |4.68664596273292   |\n",
      "|comedy-comedy-interviews         |179         |18           |3638                  |4.8                |\n",
      "|business-entrepreneurship        |999         |18           |20222                 |4.888688688688687  |\n",
      "|sports-wilderness                |900         |17           |16688                 |4.728888888888889  |\n",
      "|tv-film                          |4691        |12           |57373                 |4.722553826476235  |\n",
      "|business-careers                 |2711        |12           |41468                 |4.838694208779049  |\n",
      "|health-fitness-alternative-health|1406        |11           |22686                 |4.720981507823611  |\n",
      "|arts                             |10789       |11           |131691                |4.720724812308832  |\n",
      "|society-culture-personal-journals|2916        |10           |37232                 |4.8094307270233205 |\n",
      "+---------------------------------+------------+-------------+----------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_name</th>\n",
       "      <th>num_podcasts</th>\n",
       "      <th>total_reviews</th>\n",
       "      <th>total_ratings_recorded</th>\n",
       "      <th>avg_platform_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>10535</td>\n",
       "      <td>119</td>\n",
       "      <td>131304</td>\n",
       "      <td>4.787271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>education</td>\n",
       "      <td>10148</td>\n",
       "      <td>108</td>\n",
       "      <td>123105</td>\n",
       "      <td>4.632036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>health-fitness</td>\n",
       "      <td>7243</td>\n",
       "      <td>59</td>\n",
       "      <td>102974</td>\n",
       "      <td>4.695016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>education-self-improvement</td>\n",
       "      <td>1333</td>\n",
       "      <td>59</td>\n",
       "      <td>24694</td>\n",
       "      <td>4.840585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arts-visual-arts</td>\n",
       "      <td>1815</td>\n",
       "      <td>55</td>\n",
       "      <td>22980</td>\n",
       "      <td>4.728815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>business-investing</td>\n",
       "      <td>1925</td>\n",
       "      <td>42</td>\n",
       "      <td>27285</td>\n",
       "      <td>4.746026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>education-how-to</td>\n",
       "      <td>709</td>\n",
       "      <td>42</td>\n",
       "      <td>10877</td>\n",
       "      <td>4.707757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>society-culture</td>\n",
       "      <td>12652</td>\n",
       "      <td>39</td>\n",
       "      <td>163138</td>\n",
       "      <td>4.750277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>comedy</td>\n",
       "      <td>6449</td>\n",
       "      <td>29</td>\n",
       "      <td>82668</td>\n",
       "      <td>4.814808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sports</td>\n",
       "      <td>5789</td>\n",
       "      <td>25</td>\n",
       "      <td>77852</td>\n",
       "      <td>4.751961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>news</td>\n",
       "      <td>4418</td>\n",
       "      <td>18</td>\n",
       "      <td>55587</td>\n",
       "      <td>4.665143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>news-tech-news</td>\n",
       "      <td>966</td>\n",
       "      <td>18</td>\n",
       "      <td>12183</td>\n",
       "      <td>4.686646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>comedy-comedy-interviews</td>\n",
       "      <td>179</td>\n",
       "      <td>18</td>\n",
       "      <td>3638</td>\n",
       "      <td>4.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>business-entrepreneurship</td>\n",
       "      <td>999</td>\n",
       "      <td>18</td>\n",
       "      <td>20222</td>\n",
       "      <td>4.888689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sports-wilderness</td>\n",
       "      <td>900</td>\n",
       "      <td>17</td>\n",
       "      <td>16688</td>\n",
       "      <td>4.728889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tv-film</td>\n",
       "      <td>4691</td>\n",
       "      <td>12</td>\n",
       "      <td>57373</td>\n",
       "      <td>4.722554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>business-careers</td>\n",
       "      <td>2711</td>\n",
       "      <td>12</td>\n",
       "      <td>41468</td>\n",
       "      <td>4.838694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>health-fitness-alternative-health</td>\n",
       "      <td>1406</td>\n",
       "      <td>11</td>\n",
       "      <td>22686</td>\n",
       "      <td>4.720982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>arts</td>\n",
       "      <td>10789</td>\n",
       "      <td>11</td>\n",
       "      <td>131691</td>\n",
       "      <td>4.720725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>society-culture-personal-journals</td>\n",
       "      <td>2916</td>\n",
       "      <td>10</td>\n",
       "      <td>37232</td>\n",
       "      <td>4.809431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        category_name  num_podcasts  total_reviews  \\\n",
       "0                            business         10535            119   \n",
       "1                           education         10148            108   \n",
       "2                      health-fitness          7243             59   \n",
       "3          education-self-improvement          1333             59   \n",
       "4                    arts-visual-arts          1815             55   \n",
       "5                  business-investing          1925             42   \n",
       "6                    education-how-to           709             42   \n",
       "7                     society-culture         12652             39   \n",
       "8                              comedy          6449             29   \n",
       "9                              sports          5789             25   \n",
       "10                               news          4418             18   \n",
       "11                     news-tech-news           966             18   \n",
       "12           comedy-comedy-interviews           179             18   \n",
       "13          business-entrepreneurship           999             18   \n",
       "14                  sports-wilderness           900             17   \n",
       "15                            tv-film          4691             12   \n",
       "16                   business-careers          2711             12   \n",
       "17  health-fitness-alternative-health          1406             11   \n",
       "18                               arts         10789             11   \n",
       "19  society-culture-personal-journals          2916             10   \n",
       "\n",
       "    total_ratings_recorded  avg_platform_rating  \n",
       "0                   131304             4.787271  \n",
       "1                   123105             4.632036  \n",
       "2                   102974             4.695016  \n",
       "3                    24694             4.840585  \n",
       "4                    22980             4.728815  \n",
       "5                    27285             4.746026  \n",
       "6                    10877             4.707757  \n",
       "7                   163138             4.750277  \n",
       "8                    82668             4.814808  \n",
       "9                    77852             4.751961  \n",
       "10                   55587             4.665143  \n",
       "11                   12183             4.686646  \n",
       "12                    3638             4.800000  \n",
       "13                   20222             4.888689  \n",
       "14                   16688             4.728889  \n",
       "15                   57373             4.722554  \n",
       "16                   41468             4.838694  \n",
       "17                   22686             4.720982  \n",
       "18                  131691             4.720725  \n",
       "19                   37232             4.809431  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "# Make Pandas tables easier to read\n",
    "pd.set_option(\"display.max_colwidth\", 120)\n",
    "\n",
    "# -------------------------------------------\n",
    "# 1. Aggregate reviews at podcast level\n",
    "# -------------------------------------------\n",
    "podcast_reviews = (\n",
    "    reviews_df\n",
    "    .groupBy(\"podcast_id\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"num_reviews\"),\n",
    "        F.avg(\"rating\").alias(\"avg_review_rating_from_reviews\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# -------------------------------------------\n",
    "# 2. Core podcast info\n",
    "# -------------------------------------------\n",
    "podcast_core = (\n",
    "    podcasts_df\n",
    "    .select(\n",
    "        \"podcast_id\",\n",
    "        \"title\",\n",
    "        \"description\",\n",
    "        F.col(\"average_rating\").alias(\"avg_rating_platform\"),\n",
    "        F.col(\"ratings_count\").cast(\"long\").alias(\"rating_count\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# -------------------------------------------\n",
    "# 3. Join podcasts with review aggregates\n",
    "# -------------------------------------------\n",
    "podcast_with_engagement = (\n",
    "    podcast_core\n",
    "    .join(podcast_reviews, on=\"podcast_id\", how=\"left\")\n",
    "    .fillna({\"num_reviews\": 0})\n",
    ")\n",
    "\n",
    "# -------------------------------------------\n",
    "# 4. Clean subset: only podcasts with ratings\n",
    "#    and ensure description_length is not null\n",
    "# -------------------------------------------\n",
    "podcast_clean = (\n",
    "    podcast_with_engagement\n",
    "    .where(\n",
    "        F.col(\"avg_rating_platform\").isNotNull() &\n",
    "        F.col(\"rating_count\").isNotNull()\n",
    "    )\n",
    ")\n",
    "\n",
    "podcast_clean = (\n",
    "    podcast_clean\n",
    "    .withColumn(\"description\", F.coalesce(\"description\", F.lit(\"\")))\n",
    "    .withColumn(\"description_length\", F.length(F.col(\"description\")))\n",
    ")\n",
    "\n",
    "print(\"Sample of clean podcasts:\")\n",
    "podcast_clean.show(5, truncate=False)\n",
    "print(\"Clean podcasts:\", podcast_clean.count())\n",
    "\n",
    "# -------------------------------------------\n",
    "# 5. Attach category information\n",
    "# -------------------------------------------\n",
    "category_lookup = (\n",
    "    categories_df\n",
    "    .select(\n",
    "        \"podcast_id\",\n",
    "        F.col(\"category\").alias(\"category_name\"),\n",
    "        F.col(\"itunes_id\").alias(\"category_itunes_id\")\n",
    "    )\n",
    "    .dropDuplicates([\"podcast_id\", \"category_name\"])\n",
    ")\n",
    "\n",
    "podcast_final = (\n",
    "    podcast_clean\n",
    "    .join(category_lookup, on=\"podcast_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "print(\"Sample of podcast_final:\")\n",
    "podcast_final.show(5, truncate=False)\n",
    "print(\"Podcast_final rows:\", podcast_final.count())\n",
    "\n",
    "# -------------------------------------------\n",
    "# 6. Category-level engagement summary\n",
    "# -------------------------------------------\n",
    "category_engagement = (\n",
    "    podcast_final\n",
    "    .groupBy( \"category_name\")\n",
    "    .agg(\n",
    "        F.countDistinct(\"podcast_id\").alias(\"num_podcasts\"),\n",
    "        F.sum(\"num_reviews\").alias(\"total_reviews\"),\n",
    "        F.sum(\"rating_count\").alias(\"total_ratings_recorded\"),\n",
    "        F.avg(\"avg_rating_platform\").alias(\"avg_platform_rating\")\n",
    "    )\n",
    "    .orderBy(F.desc(\"total_reviews\"))\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "# 7. Tidy category view: drop nulls and\n",
    "#    require at least 5 podcasts per category\n",
    "# -------------------------------------------\n",
    "category_engagement_clean = (\n",
    "    category_engagement\n",
    "    .where(F.col(\"category_name\").isNotNull())\n",
    ")\n",
    "\n",
    "category_engagement_filtered = (\n",
    "    category_engagement_clean\n",
    "    .where(F.col(\"num_podcasts\") >= 5)\n",
    "    .orderBy(F.desc(\"total_reviews\"))\n",
    ")\n",
    "\n",
    "print(\"Filtered category_engagement (top 20):\")\n",
    "category_engagement_filtered.show(20, truncate=False)\n",
    "\n",
    "# -------------------------------------------\n",
    "# 8. Pretty Pandas table for top 20 categories\n",
    "# -------------------------------------------\n",
    "cat_top = category_engagement_filtered.limit(20).toPandas()\n",
    "cat_top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61027800",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "podcast_clean: 81842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 79:=======>                                                  (1 + 7) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distinct podcast_ids in podcast_final: 81842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"podcast_clean:\", podcast_clean.count())\n",
    "print(\"distinct podcast_ids in podcast_final:\",\n",
    "      podcast_final.select(\"podcast_id\").distinct().count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f30638b3-5bf6-4761-af5a-0383f129844b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== reviews_df ====\n",
      "Total rows:                5607021\n",
      "Rows with NULL podcast_id: 0\n",
      "Rows with NON-NULL id:     5607021\n",
      "Distinct podcast_id values:303911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== podcasts_df ====\n",
      "Total rows:                2077665\n",
      "Rows with NULL podcast_id: 0\n",
      "Rows with NON-NULL id:     2077665\n",
      "Distinct podcast_id values:2077665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== categories_df ====\n",
      "Total rows:                3706368\n",
      "Rows with NULL podcast_id: 0\n",
      "Rows with NON-NULL id:     3706368\n",
      "Distinct podcast_id values:2118882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reviews podcast_ids NOT found in podcasts_df: 303880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|podcast_id                      |\n",
      "+--------------------------------+\n",
      "|00017292b6ce7c58baf5e4b9f39a8065|\n",
      "|00043fd68e8b12730c9e8be75038faa6|\n",
      "|0006a430c1982e4fc338b1ab5cf011f3|\n",
      "|000b7305c3495e64673b19f705ecbcaa|\n",
      "|000b8cf1ab280459f30f81dcba1ada09|\n",
      "|000dd6d7a4fec86c442e2ce2c5cb43ac|\n",
      "|000fd1f44f43585d918f4da347c14ae1|\n",
      "|0012386a5c043c3c8dea1a6249c8f281|\n",
      "|0013008c799bc93c91d9973dc24a1e44|\n",
      "|0014daae263a6294826c640e00fcdcc8|\n",
      "+--------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reviews podcast_ids NOT found in categories_df: 303847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/25 05:00:22 ERROR Executor: Exception in task 4.0 in stage 162.0 (TID 634)\n",
      "org.apache.hadoop.fs.FSError: java.io.IOException: Input/output error\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:345)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:405)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: Input/output error\n",
      "\tat java.base/java.io.FileInputStream.readBytes(Native Method)\n",
      "\tat java.base/java.io.FileInputStream.read(FileInputStream.java:287)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\n",
      "\t... 40 more\n",
      "25/11/25 05:00:22 ERROR Executor: Exception in task 1.0 in stage 162.0 (TID 631)\n",
      "org.apache.hadoop.fs.FSError: java.io.IOException: Input/output error\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:345)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:405)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: Input/output error\n",
      "\tat java.base/java.io.FileInputStream.readBytes(Native Method)\n",
      "\tat java.base/java.io.FileInputStream.read(FileInputStream.java:287)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\n",
      "\t... 40 more\n",
      "25/11/25 05:00:22 ERROR Executor: Exception in task 0.0 in stage 162.0 (TID 630)\n",
      "org.apache.hadoop.fs.FSError: java.io.IOException: Input/output error\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:345)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:405)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: Input/output error\n",
      "\tat java.base/java.io.FileInputStream.readBytes(Native Method)\n",
      "\tat java.base/java.io.FileInputStream.read(FileInputStream.java:287)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\n",
      "\t... 40 more\n",
      "25/11/25 05:00:22 ERROR Executor: Exception in task 6.0 in stage 162.0 (TID 636)\n",
      "org.apache.hadoop.fs.FSError: java.io.IOException: Input/output error\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:345)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:405)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: Input/output error\n",
      "\tat java.base/java.io.FileInputStream.readBytes(Native Method)\n",
      "\tat java.base/java.io.FileInputStream.read(FileInputStream.java:287)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\n",
      "\t... 40 more\n",
      "25/11/25 05:00:22 WARN TaskSetManager: Lost task 0.0 in stage 162.0 (TID 630) (22a73d08c08d executor driver): org.apache.hadoop.fs.FSError: java.io.IOException: Input/output error\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:345)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:405)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: Input/output error\n",
      "\tat java.base/java.io.FileInputStream.readBytes(Native Method)\n",
      "\tat java.base/java.io.FileInputStream.read(FileInputStream.java:287)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\n",
      "\t... 40 more\n",
      "\n",
      "25/11/25 05:00:22 ERROR TaskSetManager: Task 0 in stage 162.0 failed 1 times; aborting job\n",
      "25/11/25 05:00:22 WARN TaskSetManager: Lost task 2.0 in stage 162.0 (TID 632) (22a73d08c08d executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 162.0 failed 1 times, most recent failure: Lost task 0.0 in stage 162.0 (TID 630) (22a73d08c08d executor driver): org.apache.hadoop.fs.FSError: java.io.IOException: Input/output error\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:345)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:405)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: Input/output error\n",
      "\tat java.base/java.io.FileInputStream.readBytes(Native Method)\n",
      "\tat java.base/java.io.FileInputStream.read(FileInputStream.java:287)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\n",
      "\t... 40 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/11/25 05:00:22 WARN TaskSetManager: Lost task 5.0 in stage 162.0 (TID 635) (22a73d08c08d executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 162.0 failed 1 times, most recent failure: Lost task 0.0 in stage 162.0 (TID 630) (22a73d08c08d executor driver): org.apache.hadoop.fs.FSError: java.io.IOException: Input/output error\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:345)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:405)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: Input/output error\n",
      "\tat java.base/java.io.FileInputStream.readBytes(Native Method)\n",
      "\tat java.base/java.io.FileInputStream.read(FileInputStream.java:287)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\n",
      "\t... 40 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/11/25 05:00:22 WARN TaskSetManager: Lost task 8.0 in stage 162.0 (TID 638) (22a73d08c08d executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 162.0 failed 1 times, most recent failure: Lost task 0.0 in stage 162.0 (TID 630) (22a73d08c08d executor driver): org.apache.hadoop.fs.FSError: java.io.IOException: Input/output error\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:345)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:405)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: Input/output error\n",
      "\tat java.base/java.io.FileInputStream.readBytes(Native Method)\n",
      "\tat java.base/java.io.FileInputStream.read(FileInputStream.java:287)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\n",
      "\t... 40 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/11/25 05:00:22 WARN TaskSetManager: Lost task 3.0 in stage 162.0 (TID 633) (22a73d08c08d executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 162.0 failed 1 times, most recent failure: Lost task 0.0 in stage 162.0 (TID 630) (22a73d08c08d executor driver): org.apache.hadoop.fs.FSError: java.io.IOException: Input/output error\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:345)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:405)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: Input/output error\n",
      "\tat java.base/java.io.FileInputStream.readBytes(Native Method)\n",
      "\tat java.base/java.io.FileInputStream.read(FileInputStream.java:287)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\n",
      "\t... 40 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/11/25 05:00:22 WARN TaskSetManager: Lost task 0.0 in stage 163.0 (TID 642) (22a73d08c08d executor driver): TaskKilled (Stage cancelled: Job 85 cancelled )\n",
      "25/11/25 05:00:22 WARN TaskSetManager: Lost task 7.0 in stage 162.0 (TID 637) (22a73d08c08d executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 162.0 failed 1 times, most recent failure: Lost task 0.0 in stage 162.0 (TID 630) (22a73d08c08d executor driver): org.apache.hadoop.fs.FSError: java.io.IOException: Input/output error\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:345)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:405)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: Input/output error\n",
      "\tat java.base/java.io.FileInputStream.readBytes(Native Method)\n",
      "\tat java.base/java.io.FileInputStream.read(FileInputStream.java:287)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\n",
      "\t... 40 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/11/25 05:00:22 WARN TaskSetManager: Lost task 1.0 in stage 163.0 (TID 643) (22a73d08c08d executor driver): TaskKilled (Stage cancelled: Job 85 cancelled )\n",
      "25/11/25 05:00:22 WARN TaskSetManager: Lost task 11.0 in stage 162.0 (TID 641) (22a73d08c08d executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 162.0 failed 1 times, most recent failure: Lost task 0.0 in stage 162.0 (TID 630) (22a73d08c08d executor driver): org.apache.hadoop.fs.FSError: java.io.IOException: Input/output error\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:345)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:405)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: Input/output error\n",
      "\tat java.base/java.io.FileInputStream.readBytes(Native Method)\n",
      "\tat java.base/java.io.FileInputStream.read(FileInputStream.java:287)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\n",
      "\t... 40 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/11/25 05:00:22 WARN TaskSetManager: Lost task 10.0 in stage 162.0 (TID 640) (22a73d08c08d executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 162.0 failed 1 times, most recent failure: Lost task 0.0 in stage 162.0 (TID 630) (22a73d08c08d executor driver): org.apache.hadoop.fs.FSError: java.io.IOException: Input/output error\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:345)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:405)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: Input/output error\n",
      "\tat java.base/java.io.FileInputStream.readBytes(Native Method)\n",
      "\tat java.base/java.io.FileInputStream.read(FileInputStream.java:287)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\n",
      "\t... 40 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/11/25 05:00:23 WARN TaskSetManager: Lost task 9.0 in stage 162.0 (TID 639) (22a73d08c08d executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 162.0 failed 1 times, most recent failure: Lost task 0.0 in stage 162.0 (TID 630) (22a73d08c08d executor driver): org.apache.hadoop.fs.FSError: java.io.IOException: Input/output error\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:345)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:405)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: Input/output error\n",
      "\tat java.base/java.io.FileInputStream.readBytes(Native Method)\n",
      "\tat java.base/java.io.FileInputStream.read(FileInputStream.java:287)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\n",
      "\t... 40 more\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o240.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 162.0 failed 1 times, most recent failure: Lost task 0.0 in stage 162.0 (TID 630) (22a73d08c08d executor driver): org.apache.hadoop.fs.FSError: java.io.IOException: Input/output error\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:345)\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:405)\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.io.IOException: Input/output error\n\tat java.base/java.io.FileInputStream.readBytes(Native Method)\n\tat java.base/java.io.FileInputStream.read(FileInputStream.java:287)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\n\t... 40 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: Input/output error\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:345)\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:405)\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.io.IOException: Input/output error\n\tat java.base/java.io.FileInputStream.readBytes(Native Method)\n\tat java.base/java.io.FileInputStream.read(FileInputStream.java:287)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\n\t... 40 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 48\u001b[0m\n\u001b[1;32m     43\u001b[0m missing_in_categories \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     44\u001b[0m     reviews_ids\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;241m.\u001b[39mjoin(categories_ids\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m\"\u001b[39m), on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpodcast_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft_anti\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mReviews podcast_ids NOT found in categories_df:\u001b[39m\u001b[38;5;124m\"\u001b[39m, missing_in_categories\u001b[38;5;241m.\u001b[39mcount())\n\u001b[0;32m---> 48\u001b[0m \u001b[43mmissing_in_categories\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# (optional) podcasts with no categories\u001b[39;00m\n\u001b[1;32m     51\u001b[0m podcasts_without_category \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     52\u001b[0m     podcasts_ids\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;241m.\u001b[39mjoin(categories_ids\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m\"\u001b[39m), on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpodcast_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft_anti\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:972\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    965\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    966\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    969\u001b[0m         },\n\u001b[1;32m    970\u001b[0m     )\n\u001b[0;32m--> 972\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o240.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 162.0 failed 1 times, most recent failure: Lost task 0.0 in stage 162.0 (TID 630) (22a73d08c08d executor driver): org.apache.hadoop.fs.FSError: java.io.IOException: Input/output error\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:345)\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:405)\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.io.IOException: Input/output error\n\tat java.base/java.io.FileInputStream.readBytes(Native Method)\n\tat java.base/java.io.FileInputStream.read(FileInputStream.java:287)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\n\t... 40 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: Input/output error\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:345)\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:405)\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.io.IOException: Input/output error\n\tat java.base/java.io.FileInputStream.readBytes(Native Method)\n\tat java.base/java.io.FileInputStream.read(FileInputStream.java:287)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\n\t... 40 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ---------- 1. Basic null / distinct stats per file ----------\n",
    "\n",
    "def podcast_id_stats(df, name):\n",
    "    total = df.count()\n",
    "    nulls = df.filter(F.col(\"podcast_id\").isNull()).count()\n",
    "    non_null = total - nulls\n",
    "    distinct_ids = df.select(\"podcast_id\").distinct().count()\n",
    "\n",
    "    print(f\"\\n==== {name} ====\")\n",
    "    print(f\"Total rows:                {total}\")\n",
    "    print(f\"Rows with NULL podcast_id: {nulls}\")\n",
    "    print(f\"Rows with NON-NULL id:     {non_null}\")\n",
    "    print(f\"Distinct podcast_id values:{distinct_ids}\")\n",
    "\n",
    "    if nulls > 0:\n",
    "        print(f\"\\nSample rows with NULL podcast_id in {name}:\")\n",
    "        df.filter(F.col(\"podcast_id\").isNull()).show(5, truncate=False)\n",
    "\n",
    "# run stats on each dataframe\n",
    "podcast_id_stats(reviews_df,    \"reviews_df\")\n",
    "podcast_id_stats(podcasts_df,   \"podcasts_df\")\n",
    "podcast_id_stats(categories_df, \"categories_df\")\n",
    "\n",
    "\n",
    "# ---------- 2. Check cross-dataset coverage of podcast_id ----------\n",
    "\n",
    "# distinct ids in each\n",
    "reviews_ids    = reviews_df.select(\"podcast_id\").where(F.col(\"podcast_id\").isNotNull()).distinct()\n",
    "podcasts_ids   = podcasts_df.select(\"podcast_id\").where(F.col(\"podcast_id\").isNotNull()).distinct()\n",
    "categories_ids = categories_df.select(\"podcast_id\").where(F.col(\"podcast_id\").isNotNull()).distinct()\n",
    "\n",
    "# reviews whose podcast_id is NOT present in podcasts\n",
    "missing_in_podcasts = (\n",
    "    reviews_ids.alias(\"r\")\n",
    "    .join(podcasts_ids.alias(\"p\"), on=\"podcast_id\", how=\"left_anti\")\n",
    ")\n",
    "print(\"\\nReviews podcast_ids NOT found in podcasts_df:\", missing_in_podcasts.count())\n",
    "missing_in_podcasts.show(10, truncate=False)\n",
    "\n",
    "# reviews whose podcast_id is NOT present in categories\n",
    "missing_in_categories = (\n",
    "    reviews_ids.alias(\"r\")\n",
    "    .join(categories_ids.alias(\"c\"), on=\"podcast_id\", how=\"left_anti\")\n",
    ")\n",
    "print(\"\\nReviews podcast_ids NOT found in categories_df:\", missing_in_categories.count())\n",
    "missing_in_categories.show(10, truncate=False)\n",
    "\n",
    "# (optional) podcasts with no categories\n",
    "podcasts_without_category = (\n",
    "    podcasts_ids.alias(\"p\")\n",
    "    .join(categories_ids.alias(\"c\"), on=\"podcast_id\", how=\"left_anti\")\n",
    ")\n",
    "print(\"\\nPodcasts with NO entry in categories_df:\", podcasts_without_category.count())\n",
    "podcasts_without_category.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957e1c37-b149-40b4-b61e-cea77fefe6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 120)\n",
    "\n",
    "# Make Spark a bit lighter for laptop\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"16\")\n",
    "\n",
    "# -------------------------------------------\n",
    "# 1. Prepare reviews for NLP (RANDOM SAMPLE)\n",
    "# -------------------------------------------\n",
    "BASE_REVIEWS = (\n",
    "    reviews_df\n",
    "    .filter(F.col(\"content\").isNotNull())\n",
    "    .select(\n",
    "        \"podcast_id\",\n",
    "        \"rating\",\n",
    "        F.col(\"content\").alias(\"text\")\n",
    "    )\n",
    ")\n",
    "\n",
    "MAX_REVIEWS = 2000   # you can increase to e.g. 40000 if it runs smoothly\n",
    "\n",
    "sample_reviews_spark = (\n",
    "    BASE_REVIEWS\n",
    "    .orderBy(F.rand())\n",
    "    .limit(MAX_REVIEWS)\n",
    ")\n",
    "\n",
    "total_reviews = BASE_REVIEWS.count()\n",
    "sample_reviews_count = sample_reviews_spark.count()\n",
    "print(f\"Total reviews in dataset:   {total_reviews}\")\n",
    "print(f\"Sampled reviews for BERT:   {sample_reviews_count}\")\n",
    "\n",
    "# Pull sample to Pandas (only the sample, not the full dataset)\n",
    "sample_reviews = sample_reviews_spark.toPandas()\n",
    "\n",
    "# -------------------------------------------\n",
    "# 2. Load DistilBERT sentiment model\n",
    "# -------------------------------------------\n",
    "device = 0 if torch.cuda.is_available() else -1  # GPU if available, else CPU\n",
    "sentiment_pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# -------------------------------------------\n",
    "# 3. Run model in batches and build sentiment scores\n",
    "# -------------------------------------------\n",
    "texts = sample_reviews[\"text\"].tolist()\n",
    "batch_size = 16   # reduce to 16 if you hit memory issues\n",
    "\n",
    "labels = []\n",
    "scores = []\n",
    "\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i:i + batch_size]\n",
    "    outputs = sentiment_pipe(\n",
    "        batch,\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "    for out in outputs:\n",
    "        labels.append(out[\"label\"])   # POSITIVE / NEGATIVE\n",
    "        scores.append(out[\"score\"])   # 0..1 confidence\n",
    "\n",
    "sample_reviews[\"hf_label\"] = labels\n",
    "sample_reviews[\"hf_score_raw\"] = scores\n",
    "\n",
    "def signed_score(row):\n",
    "    return row[\"hf_score_raw\"] if row[\"hf_label\"] == \"POSITIVE\" else -row[\"hf_score_raw\"]\n",
    "\n",
    "sample_reviews[\"sentiment_score\"] = sample_reviews.apply(signed_score, axis=1)\n",
    "\n",
    "print(\"Example scored reviews:\")\n",
    "display(sample_reviews[[\"rating\", \"hf_label\", \"sentiment_score\", \"text\"]].head(10))\n",
    "\n",
    "\n",
    "display(sample_reviews[[\"podcast_id\", \"rating\", \"sentiment_score\"]].head(10))\n",
    "\n",
    "# -------------------------------------------\n",
    "# 4. Back to Spark and aggregate sentiment per podcast\n",
    "# -------------------------------------------\n",
    "sample_scored_spark = spark.createDataFrame(\n",
    "    sample_reviews[[\"podcast_id\", \"rating\", \"sentiment_score\"]]\n",
    ")\n",
    "\n",
    "\n",
    "podcast_sentiment = (\n",
    "    sample_scored_spark\n",
    "    .groupBy(\"podcast_id\")\n",
    "    .agg(\n",
    "        F.avg(\"sentiment_score\").alias(\"avg_sentiment\"),\n",
    "        F.count(\"*\").alias(\"num_reviews_with_sentiment\"),\n",
    "        F.avg(\"rating\").alias(\"avg_rating_from_text_reviews\")\n",
    "    )\n",
    ")\n",
    "\n",
    "podcast_sentiment = podcast_sentiment.cache()\n",
    "print(\"podcast_sentiment rows (sample-based):\", podcast_sentiment.count())\n",
    "podcast_sentiment.show(5, truncate=False)\n",
    "\n",
    "# -------------------------------------------\n",
    "# 5. Join into main podcast table\n",
    "#     (podcast_final must already exist from previous cell)\n",
    "# -------------------------------------------\n",
    "podcast_final_with_sentiment = (\n",
    "    podcast_final\n",
    "    .join(podcast_sentiment, on=\"podcast_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "podcast_final_with_sentiment = podcast_final_with_sentiment.cache()\n",
    "\n",
    "print(\"Sample podcasts with sentiment + engagement:\")\n",
    "(\n",
    "    podcast_final_with_sentiment\n",
    "    .select(\n",
    "        \"title\",\n",
    "        \"category_name\",\n",
    "        \"avg_rating_platform\",\n",
    "        \"num_reviews\",\n",
    "        \"avg_sentiment\",\n",
    "        \"num_reviews_with_sentiment\"\n",
    "    )\n",
    "    .orderBy(F.desc(\"num_reviews_with_sentiment\"))\n",
    "    .limit(10)\n",
    "    .show(truncate=80)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374dd785-ccc9-4e49-bff5-3572d76281d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57df61b-efcb-4ec5-8fd3-01b66e0f871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"Total podcasts:\", podcast_final_with_sentiment.count())\n",
    "print(\n",
    "    \"Podcasts with sentiment:\",\n",
    "    podcast_final_with_sentiment\n",
    "        .where(F.col(\"num_reviews_with_sentiment\").isNotNull())\n",
    "        .count()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0438787f-a6ec-45b2-b1bb-16edd2b4dcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_sentiment.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3629232f-652f-420a-b932-0b955ed31685",
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_final_with_sentiment.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a779a2-4e69-438d-ab49-4fe16b58a569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"podcast_final schema podcast_id:\", podcast_final.schema[\"podcast_id\"].dataType)\n",
    "print(\"podcast_sentiment schema podcast_id:\", podcast_sentiment.schema[\"podcast_id\"].dataType)\n",
    "\n",
    "# Take one example ID that definitely exists in podcast_sentiment\n",
    "example_id = podcast_sentiment.select(\"podcast_id\").limit(1).collect()[0][0]\n",
    "print(\"Example podcast_id from sentiment:\", example_id)\n",
    "\n",
    "# Check if that ID exists in podcast_final\n",
    "exists_in_final = podcast_final.where(F.col(\"podcast_id\") == example_id).count()\n",
    "print(\"Rows with that id in podcast_final:\", exists_in_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425e2847-2178-464a-aadb-15b13c30c920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# =====================================================\n",
    "# 1. Core podcast info (from podcasts_df)\n",
    "# =====================================================\n",
    "podcast_core = (\n",
    "    podcasts_df\n",
    "    .select(\n",
    "        \"podcast_id\",\n",
    "        \"title\",\n",
    "        \"description\",\n",
    "        F.col(\"average_rating\").alias(\"avg_rating_platform\"),\n",
    "        F.col(\"ratings_count\").cast(\"long\").alias(\"rating_count\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# 2. Engagement from reviews (count + avg rating)\n",
    "# =====================================================\n",
    "podcast_reviews = (\n",
    "    reviews_df\n",
    "    .groupBy(\"podcast_id\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"num_reviews\"),\n",
    "        F.avg(\"rating\").alias(\"avg_review_rating_from_reviews\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# 3. Category lookup (1 row per podcast/category)\n",
    "# =====================================================\n",
    "category_lookup = (\n",
    "    categories_df\n",
    "    .select(\n",
    "        \"podcast_id\",\n",
    "        F.col(\"category\").alias(\"category_name\"),\n",
    "        F.col(\"itunes_id\").alias(\"category_itunes_id\")\n",
    "    )\n",
    "    .dropDuplicates([\"podcast_id\", \"category_name\"])\n",
    ")\n",
    "\n",
    "# quick sanity check\n",
    "print(\"podcast_core rows:    \", podcast_core.count())\n",
    "print(\"podcast_reviews rows: \", podcast_reviews.count())\n",
    "print(\"category_lookup rows: \", category_lookup.count())\n",
    "print(\"podcast_sentiment rows:\", podcast_sentiment.count())\n",
    "\n",
    "# =====================================================\n",
    "# 4. Build final analytic table with LEFT joins\n",
    "#    (this bypasses the old podcast_final completely)\n",
    "# =====================================================\n",
    "podcast_analytic = (\n",
    "    podcast_core\n",
    "    .join(podcast_reviews, on=\"podcast_id\", how=\"left\")\n",
    "    .join(podcast_sentiment, on=\"podcast_id\", how=\"left\")     # BERT sentiment\n",
    "    .join(category_lookup, on=\"podcast_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "# text feature\n",
    "podcast_analytic = (\n",
    "    podcast_analytic\n",
    "    .withColumn(\"description\", F.coalesce(\"description\", F.lit(\"\")))\n",
    "    .withColumn(\"description_length\", F.length(\"description\"))\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "print(\"Total podcasts in analytic table:\", podcast_analytic.count())\n",
    "\n",
    "# How many podcasts actually have BERT sentiment?\n",
    "podcasts_with_sent = podcast_analytic.where(\n",
    "    F.col(\"num_reviews_with_sentiment\").isNotNull()\n",
    ").count()\n",
    "print(\"Podcasts with sentiment (after joins):\", podcasts_with_sent)\n",
    "\n",
    "# Show only podcasts that DO have sentiment\n",
    "(\n",
    "    podcast_analytic\n",
    "    .where(F.col(\"num_reviews_with_sentiment\").isNotNull())\n",
    "    .select(\n",
    "        \"title\",\n",
    "        \"category_name\",\n",
    "        \"avg_rating_platform\",\n",
    "        \"num_reviews\",\n",
    "        \"avg_sentiment\",\n",
    "        \"num_reviews_with_sentiment\"\n",
    "    )\n",
    "    .orderBy(F.desc(\"num_reviews_with_sentiment\"))\n",
    "    .limit(10)\n",
    "    .show(truncate=80)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f41963-1dd4-4938-8b9f-6577a4dfa04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "corr = podcast_final_restricted.select(\n",
    "    \"avg_sentiment\",\n",
    "    \"avg_rating_platform\"\n",
    ").corr(\"avg_sentiment\", \"avg_rating_platform\")\n",
    "\n",
    "print(\"Correlation between sentiment and platform rating:\", corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00b5862-e03e-4bc8-85c6-1c8e0feb3c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 120)\n",
    "\n",
    "# =====================================================\n",
    "# 0. Create a unified string key: podcast_key\n",
    "# =====================================================\n",
    "\n",
    "# Reviews with unified key\n",
    "reviews_keyed = (\n",
    "    reviews_df\n",
    "    .select(\n",
    "        F.col(\"podcast_id\").cast(\"string\").alias(\"podcast_key\"),\n",
    "        \"rating\",\n",
    "        F.col(\"content\").alias(\"text\")\n",
    "    )\n",
    "    .filter(F.col(\"text\").isNotNull())\n",
    ")\n",
    "\n",
    "# Core podcast info with unified key\n",
    "podcasts_keyed = (\n",
    "    podcasts_df\n",
    "    .select(\n",
    "        F.col(\"podcast_id\").cast(\"string\").alias(\"podcast_key\"),\n",
    "        \"title\",\n",
    "        \"description\",\n",
    "        F.col(\"average_rating\").alias(\"avg_rating_platform\"),\n",
    "        F.col(\"ratings_count\").cast(\"long\").alias(\"rating_count\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Categories with unified key\n",
    "categories_keyed = (\n",
    "    categories_df\n",
    "    .select(\n",
    "        F.col(\"podcast_id\").cast(\"string\").alias(\"podcast_key\"),\n",
    "        F.col(\"category\").alias(\"category_name\"),\n",
    "        F.col(\"itunes_id\").alias(\"category_itunes_id\")\n",
    "    )\n",
    "    .dropDuplicates([\"podcast_key\", \"category_name\"])\n",
    ")\n",
    "\n",
    "print(\"reviews_keyed rows:   \", reviews_keyed.count())\n",
    "print(\"podcasts_keyed rows:  \", podcasts_keyed.count())\n",
    "print(\"categories_keyed rows:\", categories_keyed.count())\n",
    "\n",
    "# =====================================================\n",
    "# 1. BERT sentiment on a RANDOM SAMPLE of reviews_keyed\n",
    "# =====================================================\n",
    "\n",
    "TOTAL_REVIEWS = reviews_keyed.count()\n",
    "MAX_REVIEWS = 2000   # adjust upward if your laptop handles it\n",
    "\n",
    "sample_reviews_spark = (\n",
    "    reviews_keyed\n",
    "    .orderBy(F.rand())\n",
    "    .limit(MAX_REVIEWS)\n",
    ")\n",
    "\n",
    "sample_reviews = sample_reviews_spark.toPandas()\n",
    "\n",
    "print(f\"Total reviews in dataset:  {TOTAL_REVIEWS}\")\n",
    "print(f\"Sampled reviews for BERT:  {len(sample_reviews)}\")\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(\"Device set to use\", \"gpu\" if device == 0 else \"cpu\")\n",
    "\n",
    "sentiment_pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "texts = sample_reviews[\"text\"].tolist()\n",
    "batch_size = 16\n",
    "\n",
    "labels = []\n",
    "scores = []\n",
    "\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i:i + batch_size]\n",
    "    outputs = sentiment_pipe(\n",
    "        batch,\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "    for out in outputs:\n",
    "        labels.append(out[\"label\"])\n",
    "        scores.append(out[\"score\"])\n",
    "\n",
    "sample_reviews[\"hf_label\"] = labels\n",
    "sample_reviews[\"hf_score_raw\"] = scores\n",
    "\n",
    "def signed_score(row):\n",
    "    return row[\"hf_score_raw\"] if row[\"hf_label\"] == \"POSITIVE\" else -row[\"hf_score_raw\"]\n",
    "\n",
    "sample_reviews[\"sentiment_score\"] = sample_reviews.apply(signed_score, axis=1)\n",
    "\n",
    "print(\"Example scored reviews:\")\n",
    "display(sample_reviews[[\"rating\", \"hf_label\", \"sentiment_score\", \"text\"]].head(10))\n",
    "\n",
    "# =====================================================\n",
    "# 2. Back to Spark: sentiment per podcast_key\n",
    "# =====================================================\n",
    "\n",
    "sample_scored_spark = spark.createDataFrame(\n",
    "    sample_reviews[[\"podcast_key\", \"rating\", \"sentiment_score\"]]\n",
    ")\n",
    "\n",
    "podcast_sentiment_keyed = (\n",
    "    sample_scored_spark\n",
    "    .groupBy(\"podcast_key\")\n",
    "    .agg(\n",
    "        F.avg(\"sentiment_score\").alias(\"avg_sentiment\"),\n",
    "        F.count(\"*\").alias(\"num_reviews_with_sentiment\"),\n",
    "        F.avg(\"rating\").alias(\"avg_rating_from_text_reviews\")\n",
    "    )\n",
    ").cache()\n",
    "\n",
    "print(\"podcast_sentiment_keyed rows:\", podcast_sentiment_keyed.count())\n",
    "podcast_sentiment_keyed.show(5, truncate=False)\n",
    "\n",
    "# =====================================================\n",
    "# 3. Engagement from all reviews (using same key)\n",
    "# =====================================================\n",
    "\n",
    "podcast_reviews_keyed = (\n",
    "    reviews_keyed\n",
    "    .groupBy(\"podcast_key\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"num_reviews\"),\n",
    "        F.avg(\"rating\").alias(\"avg_review_rating_from_reviews\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"podcast_reviews_keyed rows:\", podcast_reviews_keyed.count())\n",
    "\n",
    "# =====================================================\n",
    "# 4. Final analytic table (ALL joins on podcast_key)\n",
    "# =====================================================\n",
    "\n",
    "podcast_analytic = (\n",
    "    podcasts_keyed\n",
    "    .join(podcast_reviews_keyed, on=\"podcast_key\", how=\"left\")\n",
    "    .join(podcast_sentiment_keyed, on=\"podcast_key\", how=\"left\")\n",
    "    .join(categories_keyed, on=\"podcast_key\", how=\"left\")\n",
    ")\n",
    "\n",
    "podcast_analytic = (\n",
    "    podcast_analytic\n",
    "    .withColumn(\"description\", F.coalesce(\"description\", F.lit(\"\")))\n",
    "    .withColumn(\"description_length\", F.length(\"description\"))\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "print(\"Total podcasts in analytic table:\", podcast_analytic.count())\n",
    "\n",
    "podcasts_with_sent = podcast_analytic.where(\n",
    "    F.col(\"num_reviews_with_sentiment\").isNotNull()\n",
    ").count()\n",
    "print(\"Podcasts with sentiment (after joins):\", podcasts_with_sent)\n",
    "\n",
    "# show only rows that actually HAVE sentiment\n",
    "(\n",
    "    podcast_analytic\n",
    "    .where(F.col(\"num_reviews_with_sentiment\").isNotNull())\n",
    "    .select(\n",
    "        \"title\",\n",
    "        \"category_name\",\n",
    "        \"avg_rating_platform\",\n",
    "        \"num_reviews\",\n",
    "        \"avg_sentiment\",\n",
    "        \"num_reviews_with_sentiment\"\n",
    "    )\n",
    "    .orderBy(F.desc(\"num_reviews_with_sentiment\"))\n",
    "    .limit(10)\n",
    "    .show(truncate=80)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c532ad-7655-4efb-afbc-a85683b2ecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 120)\n",
    "\n",
    "# =====================================================\n",
    "# 1. Build a \"reviews WITH metadata\" DataFrame\n",
    "#    (joins on podcast_key, then filter for non-null meta)\n",
    "# =====================================================\n",
    "\n",
    "# Join reviews with podcast + category meta\n",
    "reviews_with_meta = (\n",
    "    reviews_keyed.alias(\"r\")\n",
    "    .join(\n",
    "        podcasts_keyed.select(\n",
    "            \"podcast_key\",\n",
    "            \"title\",\n",
    "            \"description\",\n",
    "            \"avg_rating_platform\",\n",
    "            \"rating_count\"\n",
    "        ).alias(\"p\"),\n",
    "        on=\"podcast_key\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .join(\n",
    "        categories_keyed.select(\n",
    "            \"podcast_key\",\n",
    "            \"category_name\",\n",
    "            \"category_itunes_id\"\n",
    "        ).alias(\"c\"),\n",
    "        on=\"podcast_key\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    # keep only reviews where we actually have useful metadata\n",
    "    # .where(F.col(\"avg_rating_platform\").isNotNull())\n",
    "    # .where(F.col(\"rating_count\").isNotNull())\n",
    "    # .where(F.col(\"category_name\").isNotNull())\n",
    "    # .where(F.col(\"text\").isNotNull())\n",
    ")\n",
    "\n",
    "TOTAL_META_REVIEWS = reviews_with_meta.count()\n",
    "print(\"Total reviews WITH metadata:\", TOTAL_META_REVIEWS)\n",
    "\n",
    "# =====================================================\n",
    "# 2. SAMPLE ONLY FROM reviews_with_meta, then run BERT\n",
    "# =====================================================\n",
    "\n",
    "MAX_REVIEWS = 2000  # adjust if your machine can handle more\n",
    "\n",
    "sample_reviews_spark = (\n",
    "    reviews_with_meta\n",
    "    .orderBy(F.rand())      # still random, but from the meta subset\n",
    "    .limit(MAX_REVIEWS)\n",
    ")\n",
    "\n",
    "# keep all useful columns (podcast_key, rating, text, plus meta if you want)\n",
    "sample_reviews = sample_reviews_spark.toPandas()\n",
    "\n",
    "print(f\"Sampled reviews for BERT (with metadata): {len(sample_reviews)}\")\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(\"Device set to use\", \"gpu\" if device == 0 else \"cpu\")\n",
    "\n",
    "sentiment_pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "texts = sample_reviews[\"text\"].tolist()\n",
    "batch_size = 16\n",
    "\n",
    "labels = []\n",
    "scores = []\n",
    "\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i:i + batch_size]\n",
    "    outputs = sentiment_pipe(\n",
    "        batch,\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "    for out in outputs:\n",
    "        labels.append(out[\"label\"])\n",
    "        scores.append(out[\"score\"])\n",
    "\n",
    "sample_reviews[\"hf_label\"] = labels\n",
    "sample_reviews[\"hf_score_raw\"] = scores\n",
    "\n",
    "def signed_score(row):\n",
    "    return row[\"hf_score_raw\"] if row[\"hf_label\"] == \"POSITIVE\" else -row[\"hf_score_raw\"]\n",
    "\n",
    "sample_reviews[\"sentiment_score\"] = sample_reviews.apply(signed_score, axis=1)\n",
    "\n",
    "print(\"Example scored reviews (with metadata):\")\n",
    "display(sample_reviews[[\n",
    "    \"podcast_key\",\n",
    "    \"title\",\n",
    "    \"category_name\",\n",
    "    \"rating\",\n",
    "    \"hf_label\",\n",
    "    \"sentiment_score\",\n",
    "    \"text\"\n",
    "]].head(10))\n",
    "\n",
    "# =====================================================\n",
    "# 3. Back to Spark: sentiment per podcast_key\n",
    "#    (unchanged logic, just using the new sample_reviews)\n",
    "# =====================================================\n",
    "\n",
    "sample_scored_spark = spark.createDataFrame(\n",
    "    sample_reviews[[\"podcast_key\", \"rating\", \"sentiment_score\"]]\n",
    ")\n",
    "\n",
    "podcast_sentiment_keyed = (\n",
    "    sample_scored_spark\n",
    "    .groupBy(\"podcast_key\")\n",
    "    .agg(\n",
    "        F.avg(\"sentiment_score\").alias(\"avg_sentiment\"),\n",
    "        F.count(\"*\").alias(\"num_reviews_with_sentiment\"),\n",
    "        F.avg(\"rating\").alias(\"avg_rating_from_text_reviews\")\n",
    "    )\n",
    ").cache()\n",
    "\n",
    "print(\"podcast_sentiment_keyed rows:\", podcast_sentiment_keyed.count())\n",
    "podcast_sentiment_keyed.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1bdb29-11db-4932-9c71-c4e1beae3c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# =====================================================\n",
    "# Rebuild final analytic table: BASE = podcasts that\n",
    "# actually have reviews (podcast_reviews_keyed)\n",
    "# =====================================================\n",
    "\n",
    "podcast_analytic = (\n",
    "    podcast_reviews_keyed.alias(\"r\")              # base: has at least 1 review\n",
    "    .join(podcast_sentiment_keyed.alias(\"s\"),\n",
    "          on=\"podcast_key\", how=\"left\")           # BERT sentiment (sample)\n",
    "    .join(podcasts_keyed.alias(\"p\"),\n",
    "          on=\"podcast_key\", how=\"left\")           # metadata (title, description, ratings)\n",
    "    .join(categories_keyed.alias(\"c\"),\n",
    "          on=\"podcast_key\", how=\"left\")           # categories\n",
    ")\n",
    "\n",
    "podcast_analytic = (\n",
    "    podcast_analytic\n",
    "    .withColumn(\"description\", F.coalesce(\"description\", F.lit(\"\")))\n",
    "    .withColumn(\"description_length\", F.length(\"description\"))\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "print(\"Rows in analytic table (reviews-based):\", podcast_analytic.count())\n",
    "print(\"Distinct podcasts with any review:    \", podcast_reviews_keyed.count())\n",
    "\n",
    "podcasts_with_sent = podcast_analytic.where(\n",
    "    F.col(\"num_reviews_with_sentiment\").isNotNull()\n",
    ").count()\n",
    "print(\"Podcasts with sentiment (after joins):\", podcasts_with_sent)\n",
    "\n",
    "# Show ONLY podcasts that actually have sentiment\n",
    "(\n",
    "    podcast_analytic\n",
    "    .where(F.col(\"num_reviews_with_sentiment\").isNotNull())\n",
    "    .select(\n",
    "        \"title\",\n",
    "        \"category_name\",\n",
    "        \"avg_rating_platform\",\n",
    "        \"num_reviews\",\n",
    "        \"avg_sentiment\",\n",
    "        \"num_reviews_with_sentiment\"\n",
    "    )\n",
    "    .orderBy(F.desc(\"num_reviews_with_sentiment\"))\n",
    "    .limit(10)\n",
    "    .show(truncate=80)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d80e098-1a32-4880-93fb-162772b70125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ---------- Base reviews with a unified key ----------\n",
    "reviews_keyed = (\n",
    "    reviews_df\n",
    "    .select(\n",
    "        F.col(\"podcast_id\").cast(\"string\").alias(\"podcast_key\"),\n",
    "        \"rating\",\n",
    "        F.col(\"content\").alias(\"text\")\n",
    "    )\n",
    "    .filter(F.col(\"text\").isNotNull())\n",
    ")\n",
    "\n",
    "# ---------- Podcast metadata with unified key ----------\n",
    "podcasts_keyed = (\n",
    "    podcasts_df\n",
    "    .select(\n",
    "        F.col(\"podcast_id\").cast(\"string\").alias(\"podcast_key\"),\n",
    "        \"title\",\n",
    "        \"description\",\n",
    "        F.col(\"average_rating\").alias(\"avg_rating_platform\"),\n",
    "        F.col(\"ratings_count\").cast(\"long\").alias(\"rating_count\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# ---------- Categories (NOTE: column is 'podcastid', not 'podcast_id') ----------\n",
    "categories_keyed = (\n",
    "    categories_df\n",
    "    .select(\n",
    "        F.col(\"podcast_id\").cast(\"string\").alias(\"podcast_key\"),\n",
    "        F.col(\"category\").alias(\"category_name\"),\n",
    "        F.col(\"itunes_id\").alias(\"category_itunes_id\")\n",
    "    )\n",
    "    .dropDuplicates([\"podcast_key\", \"category_name\"])\n",
    ")\n",
    "\n",
    "# ---------- Keep ONLY reviews belonging to podcasts that have metadata ----------\n",
    "valid_podcasts = podcasts_keyed.select(\"podcast_key\").dropDuplicates()\n",
    "\n",
    "reviews_for_sentiment = (\n",
    "    reviews_keyed\n",
    "    .join(valid_podcasts, on=\"podcast_key\", how=\"inner\")\n",
    ")\n",
    "\n",
    "print(\"Total reviews (all):               \", reviews_keyed.count())\n",
    "print(\"Reviews with podcast metadata:     \", reviews_for_sentiment.count())\n",
    "print(\"Distinct podcasts with metadata:   \", valid_podcasts.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7243ab18-ef3a-411a-9fd1-dfb9e23c177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 120)\n",
    "\n",
    "TOTAL_REVIEWS_META = reviews_for_sentiment.count()\n",
    "MAX_REVIEWS = 2000   # adjust down if your laptop struggles\n",
    "\n",
    "sample_reviews_spark = (\n",
    "    reviews_for_sentiment\n",
    "    .orderBy(F.rand())\n",
    "    .limit(MAX_REVIEWS)\n",
    ")\n",
    "\n",
    "sample_reviews = sample_reviews_spark.toPandas()\n",
    "\n",
    "print(f\"Total reviews with metadata: {TOTAL_REVIEWS_META}\")\n",
    "print(f\"Sampled reviews for BERT:    {len(sample_reviews)}\")\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(\"Device set to:\", \"gpu\" if device == 0 else \"cpu\")\n",
    "\n",
    "sentiment_pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "texts = sample_reviews[\"text\"].tolist()\n",
    "batch_size = 16\n",
    "\n",
    "labels = []\n",
    "scores = []\n",
    "\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i:i + batch_size]\n",
    "    outputs = sentiment_pipe(\n",
    "        batch,\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "    for out in outputs:\n",
    "        labels.append(out[\"label\"])\n",
    "        scores.append(out[\"score\"])\n",
    "\n",
    "sample_reviews[\"hf_label\"] = labels\n",
    "sample_reviews[\"hf_score_raw\"] = scores\n",
    "\n",
    "def signed_score(row):\n",
    "    return row[\"hf_score_raw\"] if row[\"hf_label\"] == \"POSITIVE\" else -row[\"hf_score_raw\"]\n",
    "\n",
    "sample_reviews[\"sentiment_score\"] = sample_reviews.apply(signed_score, axis=1)\n",
    "\n",
    "print(\"Example scored reviews:\")\n",
    "display(sample_reviews[[\"rating\", \"hf_label\", \"sentiment_score\", \"text\"]].head(10))\n",
    "\n",
    "# Back to Spark: aggregate per podcast_key\n",
    "sample_scored_spark = spark.createDataFrame(\n",
    "    sample_reviews[[\"podcast_key\", \"rating\", \"sentiment_score\"]]\n",
    ")\n",
    "\n",
    "podcast_sentiment_keyed = (\n",
    "    sample_scored_spark\n",
    "    .groupBy(\"podcast_key\")\n",
    "    .agg(\n",
    "        F.avg(\"sentiment_score\").alias(\"avg_sentiment\"),\n",
    "        F.count(\"*\").alias(\"num_reviews_with_sentiment\"),\n",
    "        F.avg(\"rating\").alias(\"avg_rating_from_text_reviews\")\n",
    "    )\n",
    ").cache()\n",
    "\n",
    "print(\"podcast_sentiment_keyed rows:\", podcast_sentiment_keyed.count())\n",
    "podcast_sentiment_keyed.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd77c05-9d6c-48c0-ae7c-49d6cca9dab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate all reviews (for engagement)\n",
    "podcast_reviews_keyed = (\n",
    "    reviews_keyed\n",
    "    .groupBy(\"podcast_key\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"num_reviews\"),\n",
    "        F.avg(\"rating\").alias(\"avg_review_rating_from_reviews\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Build final analytic table – BASE = podcasts with review stats\n",
    "podcast_analytic = (\n",
    "    podcast_reviews_keyed.alias(\"r\")\n",
    "    .join(podcast_sentiment_keyed.alias(\"s\"),\n",
    "          on=\"podcast_key\", how=\"left\")          # BERT sentiment (subset)\n",
    "    .join(podcasts_keyed.alias(\"p\"),\n",
    "          on=\"podcast_key\", how=\"left\")          # metadata\n",
    "    .join(categories_keyed.alias(\"c\"),\n",
    "          on=\"podcast_key\", how=\"left\")          # categories\n",
    ")\n",
    "\n",
    "podcast_analytic = (\n",
    "    podcast_analytic\n",
    "    .withColumn(\"description\", F.coalesce(\"description\", F.lit(\"\")))\n",
    "    .withColumn(\"description_length\", F.length(\"description\"))\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "print(\"Rows in analytic table (reviews-based):\", podcast_analytic.count())\n",
    "\n",
    "podcasts_with_sent = podcast_analytic.where(\n",
    "    F.col(\"num_reviews_with_sentiment\").isNotNull()\n",
    ").count()\n",
    "print(\"Podcasts with sentiment (after joins):\", podcasts_with_sent)\n",
    "\n",
    "(\n",
    "    podcast_analytic\n",
    "    .where(F.col(\"num_reviews_with_sentiment\").isNotNull())\n",
    "    .select(\n",
    "        \"title\",\n",
    "        \"category_name\",\n",
    "        \"avg_rating_platform\",\n",
    "        \"num_reviews\",\n",
    "        \"avg_sentiment\",\n",
    "        \"num_reviews_with_sentiment\"\n",
    "    )\n",
    "    .orderBy(F.desc(\"num_reviews_with_sentiment\"))\n",
    "    .limit(10)\n",
    "    .show(truncate=80)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8555bdf-ccf7-4008-bd06-85a70a447a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Use only podcasts that actually have sentiment\n",
    "podcasts_with_sent = (\n",
    "    podcast_analytic\n",
    "    .where(F.col(\"avg_sentiment\").isNotNull())\n",
    "    .select(\n",
    "        \"avg_sentiment\",\n",
    "        \"avg_rating_platform\",\n",
    "        \"num_reviews\",\n",
    "        \"rating_count\",\n",
    "        \"description_length\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Rows with sentiment:\", podcasts_with_sent.count())\n",
    "\n",
    "# Simple Pearson correlations\n",
    "for col in [\"avg_rating_platform\", \"num_reviews\", \"rating_count\", \"description_length\"]:\n",
    "    corr = podcasts_with_sent.stat.corr(\"avg_sentiment\", col)\n",
    "    print(f\"corr(avg_sentiment, {col}) = {corr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a042d3-ca32-469d-a625-347847ee7068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "category_engagement = (\n",
    "    podcast_analytic\n",
    "    .groupBy(\"category_name\")\n",
    "    .agg(\n",
    "        F.countDistinct(\"podcast_key\").alias(\"num_podcasts\"),\n",
    "        F.sum(\"num_reviews\").alias(\"total_reviews\"),\n",
    "        F.sum(\"rating_count\").alias(\"total_ratings_recorded\"),\n",
    "        F.avg(\"avg_rating_platform\").alias(\"avg_platform_rating\"),\n",
    "        F.avg(\"avg_sentiment\").alias(\"avg_sentiment_sampled\")\n",
    "    )\n",
    "    .where(\"category_name IS NOT NULL\")\n",
    ")\n",
    "\n",
    "print(\"Distinct categories:\", category_engagement.count())\n",
    "\n",
    "# Top 20 by total reviews\n",
    "category_engagement.orderBy(F.desc(\"total_reviews\")).show(20, truncate=80)\n",
    "\n",
    "# If you want \"highest engagement\" as reviews per podcast:\n",
    "category_engagement.withColumn(\n",
    "    \"reviews_per_podcast\",\n",
    "    F.col(\"total_reviews\") / F.col(\"num_podcasts\")\n",
    ").orderBy(F.desc(\"reviews_per_podcast\")).show(20, truncate=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1371426b-ba87-4cfb-a4df-2589e2369452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Filter to rows that have everything we need\n",
    "model_data = (\n",
    "    podcast_analytic\n",
    "    .where(\n",
    "        (F.col(\"avg_rating_platform\").isNotNull()) &\n",
    "        (F.col(\"avg_sentiment\").isNotNull())\n",
    "    )\n",
    "    .select(\n",
    "        \"podcast_key\", \"title\", \"category_name\",\n",
    "        \"avg_rating_platform\",\n",
    "        \"avg_sentiment\",\n",
    "        \"num_reviews\",\n",
    "        \"rating_count\",\n",
    "        \"description_length\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Label: 1 = high performing, 0 = otherwise\n",
    "model_data = model_data.withColumn(\n",
    "    \"label\",\n",
    "    (F.col(\"avg_rating_platform\") >= F.lit(4.5)).cast(\"int\")\n",
    ")\n",
    "\n",
    "feature_cols = [\"avg_sentiment\", \"num_reviews\", \"rating_count\", \"description_length\"]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "model_data_assembled = assembler.transform(model_data).select(\"features\", \"label\")\n",
    "\n",
    "train_df, test_df = model_data_assembled.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(\"Train rows:\", train_df.count(), \" Test rows:\", test_df.count())\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "preds = lr_model.transform(test_df)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(preds)\n",
    "print(\"Test AUC:\", auc)\n",
    "\n",
    "# Look at coefficients (rough idea of feature importance)\n",
    "print(\"Feature order:\", feature_cols)\n",
    "print(\"Coefficients:\", lr_model.coefficients)\n",
    "print(\"Intercept:\", lr_model.intercept)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9b4c6e-3ade-4d35-acf7-fcd59661fd6f",
   "metadata": {},
   "source": [
    "VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d76778-8a89-45ec-8e99-155222bd91a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Python UDF for VADER compound score\n",
    "@F.udf(DoubleType())\n",
    "def vader_compound(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    s = analyzer.polarity_scores(text)\n",
    "    return float(s[\"compound\"])\n",
    "\n",
    "# ---- choose how big a sample you want ----\n",
    "# e.g. 50_000 reviews instead of 3_000 for BERT\n",
    "VADER_SAMPLE_SIZE = 50000\n",
    "\n",
    "base_reviews_vader = (\n",
    "    reviews_df\n",
    "      .filter(F.col(\"content\").isNotNull())\n",
    "      .select(\"podcast_id\", \"rating\", \"content\")\n",
    "      .orderBy(F.rand(42))\n",
    "      .limit(VADER_SAMPLE_SIZE)\n",
    "      .cache()\n",
    ")\n",
    "\n",
    "print(\"VADER review sample rows:\", base_reviews_vader.count())\n",
    "\n",
    "vader_scored_reviews = (\n",
    "    base_reviews_vader\n",
    "      .withColumn(\"sentiment_vader\", vader_compound(\"content\"))\n",
    ")\n",
    "\n",
    "vader_scored_reviews.show(5, truncate=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2833a0-f236-4566-a247-841938b20014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the same key used everywhere else\n",
    "vader_keyed = (\n",
    "    vader_scored_reviews\n",
    "      .withColumn(\"podcast_key\", F.concat_ws(\"||\", \"podcast_id\"))\n",
    ")\n",
    "\n",
    "podcast_sentiment_vader = (\n",
    "    vader_keyed\n",
    "      .groupBy(\"podcast_key\")\n",
    "      .agg(\n",
    "          F.avg(\"sentiment_vader\").alias(\"avg_sentiment_vader\"),\n",
    "          F.count(\"*\").alias(\"num_reviews_with_sentiment_vader\"),\n",
    "          F.avg(\"rating\").alias(\"avg_rating_from_text_reviews_vader\"),\n",
    "      )\n",
    ")\n",
    "\n",
    "print(\"podcast_sentiment_vader rows:\", podcast_sentiment_vader.count())\n",
    "podcast_sentiment_vader.show(5, truncate=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09433865-1de1-4f8a-86fa-be65ac685da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_analytic_vader = (\n",
    "    podcast_analytic\n",
    "      .join(\n",
    "          podcast_sentiment_vader,\n",
    "          on=\"podcast_key\",      # <---- changed from podcast_id\n",
    "          how=\"left\"\n",
    "      )\n",
    "      .cache()\n",
    ")\n",
    "\n",
    "print(\"Rows in analytic table (reviews-based):\", podcast_analytic_vader.count())\n",
    "\n",
    "# sanity-check how many podcasts now have VADER sentiment\n",
    "podcasts_with_vader = (\n",
    "    podcast_analytic_vader\n",
    "      .where(F.col(\"avg_sentiment_vader\").isNotNull())\n",
    "      .select(\"podcast_key\")\n",
    "      .distinct()\n",
    "      .count()\n",
    ")\n",
    "\n",
    "print(\"Distinct podcasts with VADER sentiment:\", podcasts_with_vader)\n",
    "\n",
    "podcast_analytic_vader.select(\n",
    "    \"title\",\n",
    "    \"category_name\",\n",
    "    \"avg_rating_platform\",\n",
    "    \"num_reviews\",\n",
    "    \"avg_sentiment_vader\",\n",
    "    \"num_reviews_with_sentiment_vader\"\n",
    ").orderBy(F.desc(\"num_reviews_with_sentiment_vader\")).show(10, truncate=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b12dc7c-ff31-4339-ba00-cfe5bb6f6b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Minimum number of VADER-scored reviews you want per podcast\n",
    "MIN_VADER_REVIEWS = 2\n",
    "\n",
    "# Filter to podcasts that actually have VADER sentiment and enough reviews\n",
    "podcasts_vader_filtered = (\n",
    "    podcast_analytic_vader\n",
    "        .where(F.col(\"avg_sentiment_vader\").isNotNull())\n",
    "        .where(F.col(\"num_reviews_with_sentiment_vader\") >= MIN_VADER_REVIEWS)\n",
    "        .where(F.col(\"avg_rating_platform\").isNotNull())\n",
    "        .where(F.col(\"category_name\").isNotNull())\n",
    ")\n",
    "\n",
    "print(\"Total rows in analytic table (reviews-based):\",\n",
    "      podcast_analytic_vader.count())\n",
    "print(\"Podcasts with VADER sentiment & at least\",\n",
    "      MIN_VADER_REVIEWS, \"scored reviews:\",\n",
    "      podcasts_vader_filtered.count())\n",
    "\n",
    "# Take a look at the filtered set\n",
    "podcasts_vader_filtered.select(\n",
    "    \"title\",\n",
    "    \"category_name\",\n",
    "    \"avg_rating_platform\",\n",
    "    \"num_reviews\",\n",
    "    \"avg_sentiment_vader\",\n",
    "    \"num_reviews_with_sentiment_vader\"\n",
    ").orderBy(F.desc(\"num_reviews_with_sentiment_vader\")) \\\n",
    " .show(10, truncate=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440e23fe-498d-49f4-bc78-753b53c96516",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
